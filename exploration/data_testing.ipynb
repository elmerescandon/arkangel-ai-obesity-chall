{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import export_text, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data type of each column\n",
    "def analyzeDataTypes(df):\n",
    "    continuous_vars = []\n",
    "    categorical_vars = []\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            categorical_vars.append(column)\n",
    "        else:\n",
    "            continuous_vars.append(column)\n",
    "    return continuous_vars, categorical_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data with One-hot Encoding\n",
    "# Convert categorical variable into dummy/indicator variables.\n",
    "\n",
    "# - Reduce complexity of the model with many categories\n",
    "# - Convert categorical variable into numerical variable\n",
    "# - Careful with ohe variable trap\n",
    "# - Careful with multicollinearity\n",
    "def preprocessingOneHotEncoding(df, variables):\n",
    "    if 'NObeyesdad' in variables:\n",
    "        variables.remove('NObeyesdad')\n",
    "    df_train = pd.get_dummies(df, columns=variables)\n",
    "    # Eliminate multicollinearity\n",
    "    df_train.drop('Gender_Female', axis=1, inplace=True)\n",
    "    df_train.drop('family_history_with_overweight_yes', axis=1, inplace=True)\n",
    "    df_train.drop('FAVC_no', axis=1, inplace=True)\n",
    "    df_train.drop('SMOKE_yes', axis=1, inplace=True)\n",
    "    df_train.drop('SCC_yes', axis=1, inplace=True)\n",
    "    return df_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data with label encoder\n",
    "\n",
    "# - Convert categorical variable into numerical variable\n",
    "# - Avoid dummy variable trap\n",
    "# - In this case, most of the categorical variables are ordinal\n",
    "# - Check the ones that don't have a clear order (TODO)\n",
    "def preprocessingWithLabelEncoder(df, variables):\n",
    "    if 'NObeyesdad' in variables:\n",
    "        variables.remove('NObeyesdad')\n",
    "    le = LabelEncoder()\n",
    "    for column in variables:\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data ready and separate X and Y\n",
    "def prepareData(df):\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    X = df.drop('NObeyesdad', axis=1)\n",
    "    Y = df['NObeyesdad']\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with Decision Tree Classifier\n",
    "\n",
    "# - Ideal for categorical values\n",
    "# - Easy to interpret\n",
    "# - Dataset authors recommend using a Tree Classifier: \n",
    "#   De-La-Hoz-Correa, E., Mendoza Palechor, F., De-La-Hoz-Manotas, A., Morales Ortega, R., & Sánchez Hernández, A. B., \n",
    "#   \"Obesity level estimation software based on decision trees,\" Universidad de la Costa, 2019.\n",
    "\n",
    "def trainModelDT(X_train, y_train):\n",
    "    clf = Pipeline([\n",
    "        # Standarize the data and train the model\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', tree.DecisionTreeClassifier(max_depth=30, min_samples_split=5, random_state=42))\n",
    "    ])\n",
    "    param_grid = {\n",
    "        'clf__max_depth': [10, 20, 30],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "        'clf__min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    clf = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1, verbose=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with Random Forest\n",
    "\n",
    "# - Due to the high number of categorical variables,Random Forest is a good choice\n",
    "# - It's an expansion of Decision Trees, an upgrade to a recommended classifier\n",
    "# - It's a good choice for high-dimensional data\n",
    "\n",
    "def trainModelRF (X_train, y_train):\n",
    "    # Standarize the data and train the model\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Define the parameter grid for Grid Search\n",
    "    param_grid = {\n",
    "        'clf__n_estimators': [50, 100, 200],\n",
    "        'clf__max_depth': [None, 10, 20, 30],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "        'clf__min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Initialize Grid Search\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with Gradient Boosting\n",
    "def trainModelBoosting(X_train, y_train):\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', GradientBoostingClassifier())\n",
    "    ])\n",
    "\n",
    "    # Define the parameter grid for Grid Search\n",
    "    # param_grid = {\n",
    "    #     'clf__n_estimators': [50, 100, 200],\n",
    "    #     'clf__learning_rate': [0.1, 0.01, 0.001],\n",
    "    #     'clf__max_depth': [3, 5, 7],\n",
    "    #     'clf__min_samples_split': [2, 5, 10],\n",
    "    #     'clf__min_samples_leaf': [1, 2, 4]\n",
    "    # }\n",
    "\n",
    "    # Initialize Grid Search\n",
    "    # grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous variables:  ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>FCVC</th>\n",
       "      <th>NCP</th>\n",
       "      <th>CAEC</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>CH2O</th>\n",
       "      <th>SCC</th>\n",
       "      <th>FAF</th>\n",
       "      <th>TUE</th>\n",
       "      <th>CALC</th>\n",
       "      <th>MTRANS</th>\n",
       "      <th>NObeyesdad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>24.443011</td>\n",
       "      <td>1.699998</td>\n",
       "      <td>81.669950</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.983297</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.763573</td>\n",
       "      <td>no</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.976473</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Overweight_Level_II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>Frequently</td>\n",
       "      <td>no</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>Automobile</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.711460</td>\n",
       "      <td>50.165754</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.880534</td>\n",
       "      <td>1.411685</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>1.910378</td>\n",
       "      <td>no</td>\n",
       "      <td>0.866045</td>\n",
       "      <td>1.673584</td>\n",
       "      <td>no</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Insufficient_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>20.952737</td>\n",
       "      <td>1.710730</td>\n",
       "      <td>131.274851</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>1.674061</td>\n",
       "      <td>no</td>\n",
       "      <td>1.467863</td>\n",
       "      <td>0.780199</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Obesity_Type_III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>31.641081</td>\n",
       "      <td>1.914186</td>\n",
       "      <td>93.798055</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>2.679664</td>\n",
       "      <td>1.971472</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>1.979848</td>\n",
       "      <td>no</td>\n",
       "      <td>1.967973</td>\n",
       "      <td>0.931721</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Overweight_Level_II</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender        Age    Height      Weight family_history_with_overweight  \\\n",
       "0    Male  24.443011  1.699998   81.669950                            yes   \n",
       "1  Female  18.000000  1.560000   57.000000                            yes   \n",
       "2  Female  18.000000  1.711460   50.165754                            yes   \n",
       "3  Female  20.952737  1.710730  131.274851                            yes   \n",
       "4    Male  31.641081  1.914186   93.798055                            yes   \n",
       "\n",
       "  FAVC      FCVC       NCP        CAEC SMOKE      CH2O SCC       FAF  \\\n",
       "0  yes  2.000000  2.983297   Sometimes    no  2.763573  no  0.000000   \n",
       "1  yes  2.000000  3.000000  Frequently    no  2.000000  no  1.000000   \n",
       "2  yes  1.880534  1.411685   Sometimes    no  1.910378  no  0.866045   \n",
       "3  yes  3.000000  3.000000   Sometimes    no  1.674061  no  1.467863   \n",
       "4  yes  2.679664  1.971472   Sometimes    no  1.979848  no  1.967973   \n",
       "\n",
       "        TUE       CALC                 MTRANS           NObeyesdad  \n",
       "0  0.976473  Sometimes  Public_Transportation  Overweight_Level_II  \n",
       "1  1.000000         no             Automobile        Normal_Weight  \n",
       "2  1.673584         no  Public_Transportation  Insufficient_Weight  \n",
       "3  0.780199  Sometimes  Public_Transportation     Obesity_Type_III  \n",
       "4  0.931721  Sometimes  Public_Transportation  Overweight_Level_II  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Main function \"\"\"\n",
    "# Preprocessing data\n",
    "# Load the data\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "df = df.drop('id', axis=1)\n",
    "continuous_vars, categorical_vars = analyzeDataTypes(df)\n",
    "print(\"Continuous variables: \", continuous_vars)\n",
    "df.head()\n",
    "# FCVC: Vegetables\n",
    "    # Most of distribution is between 2 and 3\n",
    "# NCP: Main meals\n",
    "# CH20: Water consumption\n",
    "# FAF: Physical activity frequency\n",
    "# TUE: Time using technology devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing with variables\n",
    "df_ohe = preprocessingOneHotEncoding(df, categorical_vars)\n",
    "X_ohe, Y_ohe = prepareData(df_ohe)\n",
    "X_train_ohe, X_test_ohe, y_train_ohe, y_test_ohe = train_test_split(X_ohe, Y_ohe, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing with label encoder\n",
    "df_encoder = preprocessingWithLabelEncoder(df, categorical_vars)\n",
    "X_encoder, Y_encoder = prepareData(df_encoder)\n",
    "X_train_encoder, X_test_encoder, y_train_encoder, y_test_encoder = train_test_split(X_encoder, Y_encoder, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    }
   ],
   "source": [
    "# Train Decision Trees\n",
    "dt_ohe = trainModelDT(X_train_ohe, y_train_ohe)\n",
    "y_pred_dt_ohe = dt_ohe.predict(X_test_ohe)\n",
    "\n",
    "dt_encoder = trainModelDT(X_train_encoder, y_train_encoder)\n",
    "y_pred_dt_encoder = dt_encoder.predict(X_test_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest\n",
    "rf_ohe = trainModelRF(X_train_ohe, y_train_ohe)\n",
    "y_pred_rf_ohe = rf_ohe.predict(X_test_ohe)\n",
    "\n",
    "rf_encoder = trainModelRF(X_train_encoder, y_train_encoder)\n",
    "y_pred_rf_encoder = rf_encoder.predict(X_test_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting\n",
    "gbt_ohe = trainModelBoosting(X_train_ohe, y_train_ohe)\n",
    "y_pred_gbt_ohe = gbt_ohe.predict(X_test_ohe)\n",
    "\n",
    "gbt_encoder = trainModelBoosting(X_train_encoder, y_train_encoder)\n",
    "y_pred_gbt_encoder = gbt_encoder.predict(X_test_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model  Accuracy  Precision    Recall  F1-Score\n",
      "0   DT - One-hot Encoding  0.876927   0.877675  0.876927  0.877212\n",
      "1      DT - Label Encoder  0.875000   0.875874  0.875000  0.875342\n",
      "2   RF - One-hot Encoding  0.896195   0.896459  0.896195  0.896103\n",
      "3     RF - Label Enconder  0.899807   0.900026  0.899807  0.899763\n",
      "4  GBT - One-hot Encoding  0.903902   0.903967  0.903902  0.903917\n",
      "5     GBT - Label Encoder  0.902697   0.902877  0.902697  0.902777\n"
     ]
    }
   ],
   "source": [
    "# Compare the results\n",
    "results = pd.DataFrame({'Model': ['DT - One-hot Encoding', 'DT - Label Encoder', \n",
    "                                  'RF - One-hot Encoding', 'RF - Label Enconder',\n",
    "                                  'GBT - One-hot Encoding', 'GBT - Label Encoder'],})\n",
    "results['Accuracy'] = [accuracy_score(y_test_ohe, y_pred_dt_ohe), \n",
    "                      accuracy_score(y_test_encoder, y_pred_dt_encoder),\n",
    "                      accuracy_score(y_test_ohe, y_pred_rf_ohe),\n",
    "                      accuracy_score(y_test_encoder, y_pred_rf_encoder),\n",
    "                      accuracy_score(y_test_ohe, y_pred_gbt_ohe),\n",
    "                      accuracy_score(y_test_encoder, y_pred_gbt_encoder)]\n",
    "\n",
    "results['Precision'] = [classification_report(y_test_ohe, y_pred_dt_ohe, output_dict=True)['weighted avg']['precision'], \n",
    "                        classification_report(y_test_encoder, y_pred_dt_encoder, output_dict=True)['weighted avg']['precision'],\n",
    "                        classification_report(y_test_ohe, y_pred_rf_ohe, output_dict=True)['weighted avg']['precision'], \n",
    "                        classification_report(y_test_encoder, y_pred_rf_encoder, output_dict=True)['weighted avg']['precision'],\n",
    "                        classification_report(y_test_ohe, y_pred_gbt_ohe, output_dict=True)['weighted avg']['precision'],\n",
    "                        classification_report(y_test_encoder, y_pred_gbt_encoder, output_dict=True)['weighted avg']['precision']]\n",
    "\n",
    "\n",
    "results['Recall'] = [classification_report(y_test_ohe, y_pred_dt_ohe, output_dict=True)['weighted avg']['recall'], \n",
    "                     classification_report(y_test_encoder, y_pred_dt_encoder, output_dict=True)['weighted avg']['recall'],\n",
    "                        classification_report(y_test_ohe, y_pred_rf_ohe, output_dict=True)['weighted avg']['recall'], \n",
    "                        classification_report(y_test_encoder, y_pred_rf_encoder, output_dict=True)['weighted avg']['recall'],\n",
    "                        classification_report(y_test_ohe, y_pred_gbt_ohe, output_dict=True)['weighted avg']['recall'],\n",
    "                        classification_report(y_test_encoder, y_pred_gbt_encoder, output_dict=True)['weighted avg']['recall']]\n",
    "\n",
    "results['F1-Score'] = [classification_report(y_test_ohe, y_pred_dt_ohe, output_dict=True)['weighted avg']['f1-score'], \n",
    "                       classification_report(y_test_encoder, y_pred_dt_encoder, output_dict=True)['weighted avg']['f1-score'],\n",
    "                        classification_report(y_test_ohe, y_pred_rf_ohe, output_dict=True)['weighted avg']['f1-score'], \n",
    "                        classification_report(y_test_encoder, y_pred_rf_encoder, output_dict=True)['weighted avg']['f1-score'],\n",
    "                        classification_report(y_test_ohe, y_pred_gbt_ohe, output_dict=True)['weighted avg']['f1-score'],\n",
    "                        classification_report(y_test_encoder, y_pred_gbt_encoder, output_dict=True)['weighted avg']['f1-score']]\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: One-hot Encoding\n",
      "0.8769267822736031\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.92      0.93      0.92       524\n",
      "      Normal_Weight       0.84      0.83      0.83       626\n",
      "     Obesity_Type_I       0.87      0.84      0.86       543\n",
      "    Obesity_Type_II       0.97      0.95      0.96       657\n",
      "   Obesity_Type_III       1.00      1.00      1.00       804\n",
      " Overweight_Level_I       0.72      0.72      0.72       484\n",
      "Overweight_Level_II       0.75      0.78      0.76       514\n",
      "\n",
      "           accuracy                           0.88      4152\n",
      "          macro avg       0.86      0.86      0.86      4152\n",
      "       weighted avg       0.88      0.88      0.88      4152\n",
      "\n",
      "Decision Tree: Label Encoder\n",
      "0.875\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.91      0.92      0.92       524\n",
      "      Normal_Weight       0.83      0.82      0.83       626\n",
      "     Obesity_Type_I       0.87      0.84      0.86       543\n",
      "    Obesity_Type_II       0.97      0.96      0.96       657\n",
      "   Obesity_Type_III       1.00      1.00      1.00       804\n",
      " Overweight_Level_I       0.72      0.71      0.72       484\n",
      "Overweight_Level_II       0.74      0.78      0.76       514\n",
      "\n",
      "           accuracy                           0.88      4152\n",
      "          macro avg       0.86      0.86      0.86      4152\n",
      "       weighted avg       0.88      0.88      0.88      4152\n",
      "\n",
      "Random Forest: One-hot Encoding\n",
      "0.8961946050096339\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.94      0.91      0.93       524\n",
      "      Normal_Weight       0.83      0.89      0.86       626\n",
      "     Obesity_Type_I       0.89      0.87      0.88       543\n",
      "    Obesity_Type_II       0.97      0.98      0.97       657\n",
      "   Obesity_Type_III       1.00      1.00      1.00       804\n",
      " Overweight_Level_I       0.79      0.75      0.77       484\n",
      "Overweight_Level_II       0.79      0.80      0.80       514\n",
      "\n",
      "           accuracy                           0.90      4152\n",
      "          macro avg       0.89      0.88      0.89      4152\n",
      "       weighted avg       0.90      0.90      0.90      4152\n",
      "\n",
      "Random Forest: Label Encoder\n",
      "0.8998073217726397\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.95      0.92      0.93       524\n",
      "      Normal_Weight       0.84      0.89      0.87       626\n",
      "     Obesity_Type_I       0.89      0.87      0.88       543\n",
      "    Obesity_Type_II       0.97      0.97      0.97       657\n",
      "   Obesity_Type_III       1.00      1.00      1.00       804\n",
      " Overweight_Level_I       0.78      0.76      0.77       484\n",
      "Overweight_Level_II       0.80      0.81      0.81       514\n",
      "\n",
      "           accuracy                           0.90      4152\n",
      "          macro avg       0.89      0.89      0.89      4152\n",
      "       weighted avg       0.90      0.90      0.90      4152\n",
      "\n",
      "Gradient Boosting: One-hot Encoding\n",
      "0.9039017341040463\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.95      0.94      0.94       524\n",
      "      Normal_Weight       0.87      0.89      0.88       626\n",
      "     Obesity_Type_I       0.88      0.88      0.88       543\n",
      "    Obesity_Type_II       0.98      0.97      0.97       657\n",
      "   Obesity_Type_III       1.00      1.00      1.00       804\n",
      " Overweight_Level_I       0.78      0.78      0.78       484\n",
      "Overweight_Level_II       0.80      0.80      0.80       514\n",
      "\n",
      "           accuracy                           0.90      4152\n",
      "          macro avg       0.89      0.89      0.89      4152\n",
      "       weighted avg       0.90      0.90      0.90      4152\n",
      "\n",
      "Gradient Boosting: Label Encoder\n",
      "0.9026974951830443\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.95      0.94      0.94       524\n",
      "      Normal_Weight       0.88      0.88      0.88       626\n",
      "     Obesity_Type_I       0.88      0.87      0.88       543\n",
      "    Obesity_Type_II       0.97      0.97      0.97       657\n",
      "   Obesity_Type_III       1.00      1.00      1.00       804\n",
      " Overweight_Level_I       0.78      0.78      0.78       484\n",
      "Overweight_Level_II       0.80      0.80      0.80       514\n",
      "\n",
      "           accuracy                           0.90      4152\n",
      "          macro avg       0.89      0.89      0.89      4152\n",
      "       weighted avg       0.90      0.90      0.90      4152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree: One-hot Encoding\")\n",
    "print(accuracy_score(y_test_ohe, y_pred_dt_ohe))\n",
    "print(classification_report(y_test_ohe, y_pred_dt_ohe))\n",
    "\n",
    "print(\"Decision Tree: Label Encoder\")\n",
    "print(accuracy_score(y_test_encoder, y_pred_dt_encoder))\n",
    "print(classification_report(y_test_encoder, y_pred_dt_encoder))\n",
    "\n",
    "\n",
    "print(\"Random Forest: One-hot Encoding\")\n",
    "print(accuracy_score(y_test_ohe, y_pred_rf_ohe))\n",
    "print(classification_report(y_test_ohe, y_pred_rf_ohe))\n",
    "\n",
    "print(\"Random Forest: Label Encoder\")\n",
    "print(accuracy_score(y_test_encoder, y_pred_rf_encoder))\n",
    "print(classification_report(y_test_encoder, y_pred_rf_encoder))\n",
    "\n",
    "print(\"Gradient Boosting: One-hot Encoding\")\n",
    "print(accuracy_score(y_test_ohe, y_pred_gbt_ohe))\n",
    "print(classification_report(y_test_ohe, y_pred_gbt_ohe))\n",
    "\n",
    "print(\"Gradient Boosting: Label Encoder\")\n",
    "print(accuracy_score(y_test_encoder, y_pred_gbt_encoder))\n",
    "print(classification_report(y_test_encoder, y_pred_gbt_encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
